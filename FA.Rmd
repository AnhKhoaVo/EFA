---
title: "FA"
author: "Anh Khoa Vo"
date: "November 9, 2017"
output:
  html_document: default
---

We have found that Albumin at week 4 is predictive of outcome - motor scores by week 52. However, I was wondering if a set of observed variables would have a better chance of predicting outcome than just one single variable. 


Exploratory Factor Analysis (EFA) has been used to explore the possible underlying factor structure of a set of observed variables without imposing a preconcieved structure on the outcome. By performing EFA, the underlying factor structure is identified. 

First, I upload the data set, and scale every variable in the dataset to reduce large variance between variable. I also remove two variables in the dataset since these two have the most missing data. (This led to several problems for my EFA later on) 

```{r, include=FALSE}
library(readxl)
cfa_week0 <- read_excel("C:/Users/Anh Khoa/Desktop/cfa-week0.xlsx")
View(cfa_week0)

cfa_week0$DAN00 <- NULL
cfa_week0$AMY00 <- NULL

cfa1_week0 <- scale(cfa_week0, center = TRUE, scale = TRUE)
cfa1_week0 <- data.frame(cfa1_week0)

str(cfa1_week0)
```

To identify the correct number of factors to pick for analysis, we can visualize it using scree plot. 

```{r}
library(psych)
scree <- fa.parallel(cfa1_week0[-1:-3], fm = "ml", fa = "fa")
scree
```

To visualize the number of factors and the variables inside each factor 

```{r}
fa <- fa(cfa1_week0[-1:-3], nfactors=7, rotate="varimax", fm="ml")
fa.diagram(fa, main = "Factor Analysis at week 0", cut = 0.4)


print(fa$loadings, cutoff = 0.5)
```

A second part of factor analysis is called Confirmatory Factor Analysis. Just like the name implies, this process is to confirm what we explored earlier. 

For the regression, we're interested in the motor scores at week 52: 

# Motor scores at week 52

```{r}
library(lavaan)
model0 <- 'Factor 1 =~ HGB00 + HCT00 + RBC00 
           Factor 2 =~ STP00 + ALB00 + CAB00 + CHO00
           Factor 3 =~ CK000 + ALT00 + AST00
           Factor 4 =~ BUN00 + BC900 + BUA00 + P0400
           Factor 5 =~ MCV00 + MCH00
           Factor 6 =~ DDN00 + DBN00 + DJN00
           AST00 ~~ 0*AST00
           MCV00 ~~ 0*MCV00
           DDN00 ~~ 0*DDN00
           TLAMS52 ~ Factor 1 + Factor 2 + Factor 3 + Factor 4 + Factor 5 + Factor 6'

f1_week0 <- sem(model0, data = cfa1_week0, std.lv=TRUE, missing = "fiml")
summary(f1_week0, standardized = TRUE, fit.measures = TRUE)

```

The output comes out with a summary table and visual graph 

```{r}
library(semPlot)
semPaths(f1_week0,"std", whatLabels="std", intercepts=FALSE, style="lisrel", residuals = FALSE,
         nCharNodes=0, 
         nCharEdges=0,
         curveAdjacent = TRUE,title=TRUE, layout="tree2",curvePivot=FALSE)
```

Now, to run the analysis, I need to compute the estimated values for latent variables!!!! 

```{r}
p_week0 <- lavPredict(f1_week0)
p_week0 <- as.data.frame(p_week0)
colnames(p_week0) <- c("Factor1_w0", "Factor2_w0", "Factor3_w0", "Factor4_w0", "Factor5_w0", "Factor6_w0")

```

And now, In order to see which factor is predictive of outcome, I run a simple linear regression of very factors to Motor scores

```{r, include=FALSE}
cfa2_week0 <- cbind(cfa1_week0, p_week0)
cfa2_week0
```


```{r}
factor_lm <- function(x) {
  f1 <- summary(lm(TLAMS52 ~ Factor1_w0, data = x))
  f2 <- summary(lm(TLAMS52 ~ Factor2_w0, data = x))
  f3 <- summary(lm(TLAMS52 ~ Factor3_w0, data = x))
  f4 <- summary(lm(TLAMS52 ~ Factor4_w0, data = x))
  f5 <- summary(lm(TLAMS52 ~ Factor5_w0, data = x))
  f6 <- summary(lm(TLAMS52 ~ Factor6_w0, data = x))
  f7 <- summary(lm(TLAMS52 ~ CHC00, data=x))
  return(list(f1,f2,f3,f4,f5,f6,f7))
}

factor_lm(cfa2_week0)
```
From this, we have 3 factors are significantly correlated to motor scores at week 52.

BUT, how about each variable made up of that factor? Does each of them correlate to outcome too??



Here is the correlation graph:
```{r}
library(knitr)
clusters <- c("Factor1_w0", "Factor2_w0", "Factor3_w0", "RBC00", "HGB00", "HCT00", "STP00", "ALB00", "CAB00", "CHO00", "CHC00", "AST00", "ALT00", "CK000")
factors <- subset(cfa2_week0, select=(clusters))
factors_cor <- cor(factors, use = "pairwise.complete.obs")

library(corrplot)
corrplot(factors_cor, method = "color", type = "upper")

```

Here is the correlation coffiecients values:

```{r}
factors <- as.data.frame(cor(factors, use="pairwise.complete.obs"))
Values <- kable(factors[1:4])
Values
```


And it seems like each of the variables made of the factor correlate to the outcome too, which is expected. 

I would like to see if other variables (not included in the EFA) are correlated to TLAMS52. 

Here is the correlation graph:
```{r}
library(knitr)
LeftOut <- c("HGB00","HCT00","RBC00","STP00","ALB00","CAB00","CHO00",
             "AST00","CK000","ALT00","BUN00","BC900","BUA00","P0400",
             "MCV00","MCH00", "CHC00", "TLAMS52", "TLAMS00", "TLAMS01")



LeftOutVars <- cfa2_week0[ , !(names(cfa2_week0) %in% LeftOut)]

CorLeftOut <- cor(LeftOutVars, use="pairwise.complete.obs")
corrplot(CorLeftOut, method = "color", type = "upper")

```

Here is the correlation table: 
```{r}
CorLeftOut <- as.data.frame(cor(LeftOutVars, use = "pairwise.complete.obs"))
CorLeftOut <- kable(CorLeftOut)
CorLeftOut
```

So those variables not included in the EFA model are not related to Motor Scores!! 

I am now interested in seeing if these 3 factors (Factor 1, 2, and 3) are significant holding other independent variables constant: 
```{r}
MS52_raw <- cfa_week0[1]
Other_Variables <- read_excel("C:/Users/Anh Khoa/Desktop/Other-Variables.xlsx")

Factors_W0 <- do.call("cbind", list(MS52_raw, p_week0, Other_Variables))

library(dplyr)
Factors_W0 <- Factors_W0 %>% mutate_if(is.character,as.factor) #They only take factor values
Factors_W0$SEXCD <- as.factor(Factors_W0$SEXCD) 
Factors_W0$DEATHRPD <- as.factor(Factors_W0$DEATHRPD)
Factors_W0$AGE <- as.numeric(Factors_W0$AGE)

```


```{r}
lm_week0_factor1 <- lm(TLAMS52 ~  Factor1_w0 + SEXCD + SPLVL1 + AGE + ASIMPC01_A, data = Factors_W0)

lm_week0_factor2 <- lm(TLAMS52 ~  Factor2_w0 + SEXCD + SPLVL1 + AGE + ASIMPC01_A, data = Factors_W0)

lm_week0_factor3 <- lm(TLAMS52 ~  Factor3_w0 + SEXCD + SPLVL1 + AGE + ASIMPC01_A, data = Factors_W0)

list(summary(lm_week0_factor1), summary(lm_week0_factor2), summary(lm_week0_factor3))
```

From the results output, it seems like factor 1 (lm_week0_factor1) is the only significant variable when holding other independent predictors constant. 

#Model Selecion
I am now interested in seeing if lm_week0_factors is significant comparing to model consisiting of only 3 significant factors and model consisting of only patients injury: 

```{r}
lm_week0_factors <- lm(TLAMS52 ~  Factor1_w0 + Factor2_w0 + Factor3_w0, data = Factors_W0)

lm_week0_inj <- lm(TLAMS52 ~ SEXCD + SPLVL1 + AGE + ASIMPC01_A, data = Factors_W0)

anova(lm_week0_factor1, lm_week0_factors, lm_week0_inj)
```

The anova results table indicate there is signifcant differences between 3 models (comparing step-wise: model 1 vs model 2, and model 2 vs model 3). Looking at RSS (Residuals Sum of Squares), it seems as if model 1, and model 3 are a better fit than model 2. However, what about model 1 vs model 3?

```{r}
anova(lm_week0_factor1, lm_week0_inj)

```

The anova results indicate there is a significant difference between these 2 models. Looking at the RSS, Model 1 seems to have a better fit than model 2. 

#Variable Selection
I am now interested in if all the variables in the previous Model 1 are relevant: 

```{r}
library(MASS)
stepAIC(lm_week0_factor1, direction = "both")
```

As the output suggested, it is as if Sex is not significant to be included, hence, I upated the model: 

```{r}
lm_week0_factor1_updated <- lm(TLAMS52 ~  Factor1_w0 + SPLVL1 + AGE + ASIMPC01_A, data = Factors_W0)
summary(lm_week0_factor1_updated)
```

Factor 1 p-values is reduced, but still significant (0.05). But would an updated model be better than the original model? Let's see an anova: 

```{r}
anova(lm_week0_factor1, lm_week0_factor1_updated, test = "Chi")
```

There is no significant difference present between the original and updated models.
